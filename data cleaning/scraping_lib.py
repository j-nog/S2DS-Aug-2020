from urllib.parse import urlparse
import  urllib
import mimetypes

import warnings,datetime,os

from web_scraping_lib import *

#package dependence for line for line formatter
import numpy as np
import difflib

# TODO: for the place holder pdf scraper only. Remove if no longer needed
import pdfminer.high_level
from pdfminer.layout import LTChar, LTTextBoxHorizontal


def create_folder(input_path="input"):
    """
    Create a data folder, if it does not exist, and name it as input
    """
    current_path = os.getcwd()
    if not os.path.exists(input_path):
        os.makedirs(input_path)

def scrape_pdf(filepath):
    """
    Rudimentary PDF scraping.
    TODO: To be replaced with the PDF scraper that segments text.
    """
    title = os.path.basename(filepath)
    text_dump = pdfminer.high_level.extract_text(filepath)

    text_segmented = pdf_dict_to_outputformat(scrape_pdf_by_line(filepath))
    contents_dict = {
        "title" : title,
        "text_dump" : text_dump,
        "text_segmented" : text_segmented,
        "hyperlinks_dump" : []
    }

    return contents_dict

def scrape_pdf_by_line(filename):
    """
    Functions for creating dicts from pdf files for
    backwards engineering the layout structure from the pdfminer.six layout
    object (LTTextBox etc. see tree in:
    https://pdfminersix.readthedocs.io/en/latest/topic/converting_pdf_to_text.html
    """
    file = open(filename,'rb')
    #initiate dataframe
    columns = [
            'page_no',
            'box_no',
            'box_pos',
            'line_no',
            'line_pos',
#            'previous_linefontsize',
            'curr_linefontsize',
            'text'
    ]
    data = {key : [] for key in columns}
    #     fontsize = None #Set fontsize to none for previous_linefontsize on 1st entry
    pagenumber = 0

    # parse over all textboxes and lines contained in them
    for page_layout in pdfminer.high_level.extract_pages(file):
        pagenumber += 1
        for element in page_layout:
            if isinstance(element, LTTextBoxHorizontal):
                lineno=0
                for text_line in element:
                    lineno += 1
                    #output line data
                    data['page_no'].append(pagenumber)
                    data['box_no'].append(element.index)
                    data['box_pos'].append(element.bbox)
                    data['line_no'].append(lineno)
                    #                     data['previous_linefontsize'].append(fontsize)
                    data['line_pos'].append(text_line.bbox)
                    data['text'].append(text_line.get_text())
                    if not text_line.get_text().isspace():
                        #Recording fontsize of first nonspace character in line
                        for character in text_line:
                            if isinstance(character,LTChar):
                                if character.get_text().isspace() :
                                    continue
                                else:
                                    fontsize = character.size
                                    data['curr_linefontsize'].append(fontsize)
                                    break
                    else:
                        data['curr_linefontsize'].append(None)
    #         if pagenumber>maxpage and maxpage != False:
    #             break
    return data

def pdf_dict_to_outputformat(pdf_dict):
    """
    Takes a dict passed by scraped_pdf_by_line() with columns [pages numbers, box
    numbers, box positions, line numbers, currentline fontsize (non-spacelike),
    text of line] and sections the text into a dataframe containing separate text
    boxes for main_text, titles, subtitles etc. and numbers them correctly
    """
    df = pd.DataFrame(pdf_dict)
    #finding main text fontsize by counting most used text font for each line
    fontcount = df['line_no'].groupby(df['curr_linefontsize'].round(decimals = 0)).count()
    textsize = fontcount.idxmax()

    #Creating indexes for labeling fontsizes in the documents as text_type.
    #Labels for fontsizes are automaticaly generated by comparing the size of the text
    #(... < subsubtext < subtext < text < subsub..title < ...< title)
    fontsizes = df['curr_linefontsize'].groupby(df['curr_linefontsize'].round(decimals = 0)).unique().keys()
    fontkeys = ['main_text']
    supindex = fontsizes > textsize
    subindex = fontsizes < textsize
    for count in range(sum(supindex)):
        fontkeys.append((sum(supindex)-count-1)*'sub'+'title')
    for count in range(sum(subindex)):
        fontkeys.insert(0,(count+1)*'sub'+'text')
    fontsizes = fontsizes.append(pd.Index([np.nan]))
    fontkeys.append(np.nan)

    #Creating extra column containing text_type
    def compare_font(entry):
        fontsize_entry = fontsizes.get_loc(round(entry,0))
        return(fontkeys[fontsize_entry])
    df['text_type'] = df['curr_linefontsize'].apply(compare_font)

    #Creating extra columns to fix cutting of sentences due to PDFminers selection
    #of horizontal textboxes (see documentation)
    df[['x0_ln','y0_ln','x1_ln','y1_ln']] = pd.DataFrame(df['line_pos'].tolist())
    df.sort_values(by=['page_no','y0_ln','x0_ln'],ascending = [True,False,True],ignore_index=True,inplace=True)


    no_breaks = df[df['text_type'].notnull()].round({'y0_ln':1,'x0_ln':1,'y1_ln':1,'x1_ln':1})

    # Kills bottom line by sorting and aligning by bottom left coordinate of
    # sentence box. If lines are very similar (overlap) measured by tolerance
    # drop the lines.

    def check_empty_iterator(iterator):
        try:
            next(iterator)
        except StopIteration:
            pass

    def kill_footers(tolerance = 0.9):
        bottom_ln = no_breaks.round({'y0_ln':1})['y0_ln'].min()
        to_match = no_breaks[no_breaks.y0_ln==bottom_ln].sort_values(by='x0_ln')['text']
        it1 = to_match.iteritems()
        it2 = to_match.iteritems()
        overlap = []
        check_empty_iterator(it2)
        for step in range(len(to_match)-1):
            value = next(it1)
            next_value = next(it2)
            match = difflib.SequenceMatcher(None,value[1],next_value[1])
            overlap.append(match.ratio())
        if overlap:
            if(sum(overlap)/len(overlap)) > tolerance:
                df.drop(to_match.index,inplace = True)
        else:
            print('no footer with tolerance level {}'.format(tolerance))

    def kill_headers(tolerance = 0.9):
        top_ln = no_breaks.round({'y1_ln':1})['y1_ln'].max()
        to_match = no_breaks[no_breaks.y1_ln==top_ln].sort_values(by='x1_ln')['text']
        it1 = to_match.iteritems()
        it2 = to_match.iteritems()
        overlap = []
        check_empty_iterator(it2)
        for step in range(len(to_match)-1):
            value = next(it1)
            next_value = next(it2)
            match = difflib.SequenceMatcher(None,value[1],next_value[1])
            overlap.append(match.ratio())
        if overlap:
            if(sum(overlap)/len(overlap)) > tolerance:
                df.drop(to_match.index,inplace = True)
        else:
            print('no header with tolerance level {}'.format(tolerance))
    kill_footers()
    kill_headers()

    #Add collumn tracking if type of text_type changes
    def riffle_repeated_type():
        notnull_df = df[df['text_type'].notnull()]
        return (notnull_df[['text_type']] == notnull_df[['text_type']].shift()).any(axis=1)
    df['text_type'] = df['text_type'].fillna(method= 'bfill').fillna(method = 'ffill')
    text_type_change = ~riffle_repeated_type().reindex(df.index).fillna(True)
    text_type_change[0] = False
    df['type_change'] = text_type_change

    def create_section_index():
        """
        Create new columns automaticaly for indexing (integer) levels of sub(secitons)
        in text.
        """
        notnull_df = df[df['text_type'].notnull()]
        #create sorted column names containing text_type unequal to main_text
        u_index = df.sort_values(by = 'curr_linefontsize')\
                 .groupby(notnull_df['curr_linefontsize'].round(decimals = 0))\
                 ['text_type'].unique().drop(textsize)
        u_index = np.array([u_index.iloc[ntry][0] for ntry in range(u_index.size)])
        sec_counters = []
        [sec_counters.append(0) for section_level in range(len(u_index))]
        counter_df = pd.DataFrame([sec_counters],columns = u_index)
        # appending to df
        df[u_index] = counter_df #pd.concat([df,counter_df],axis=1)
        df[u_index] = df[u_index].ffill(0)
        return u_index

    def iter_the_cols(df_in,list_ind):
        """
        Adds 1 to the column generatied by df.create_secton_index()
        (say subsection) if a change of text type occurs (say from
        susection to main_text). Total section index obtained by cumsum
        over these columns
        """
        if df_in['type_change'] == True:
            if df_in['text_type'] in df_in.keys():
                level = np.where(list_ind==df_in['text_type'])[0]
                df_in[df_in['text_type']] =1
                for entry in range(level[0]):
                    df_in[list_ind[entry]]=0
                #df_in[list_ind]=df_in[list_ind].ffill()
        return df_in
    #finishing up numbering of sections
    u_index = create_section_index()
    df = df.apply(iter_the_cols,list_ind=u_index,axis=1)
    df[u_index[-1]]=df[u_index[-1]].cumsum()
    for depth in range(len(u_index)-1):
        df[u_index[-2-depth]]=df.groupby(u_index[-1-depth])[u_index[-2-depth]].cumsum()

    #creating output format
    out_keys = list(u_index)
    out_keys.reverse()
    out_keys.insert(0,'page_no')
    out_keys.append('text')
    out_keys.append('text_type')
    df_out = df[out_keys]
    df_out2 = df_out.groupby(['title','subtitle','subsubtitle','text_type'],sort=False)['text'].sum().reset_index()
    return df_out2

def scrape_documents(urls, verbose = True, out_path = "pdfs/"):
    """
    Fetches a list of online documents (pdf or html) and scrapes them.
    urls : list of str
    verbose : bool
    out_path : str
        directory into which the pdfs are written
    returns : list of dict
        A list of dictionaries each holding the scraped contents
    """
    scraped_contents = []
    create_folder(out_path)
    for (ii,url) in enumerate(urls):
        if verbose:
            print("scraping " + str(ii + 1) + "/" + str(len(urls)) + ": " + url)


        media_type = get_media_type(url)

        # dictionary holding the scraped data for a document
        scrape_dict = {
                     "URL" : url,
                     "media type" : str(media_type),
                     "title" : "",
                     "text_dump" : "",
                     "text_segmented" : [],
                     "hyperlinks_dump" : [],
                     "timestamp" : None
                    }

        # keys for the entries from the outputs of the html and pdf scraping pipelines
        # to be used
        keys = ["title","text_dump","text_segmented","hyperlinks_dump"]

        # check if the URL points to a html or pdf document and call the respective scrapers
        if media_type == "application/pdf":
            # fetch the pdf to disk and scrape it
            pdf_filename = url.split('/')[-1]
            urllib.request.urlretrieve(url,out_path + pdf_filename)
            scrape_dict["timestamp"] = timestamp_utc()
            dummy_dict = scrape_pdf(out_path + pdf_filename)

            # extract the relevant entries
            scrape_dict.update(
                {key : dummy_dict[key] for key in
                 ["title","text_dump","text_segmented","hyperlinks_dump"]
                }
            )

        elif media_type == "text/html":
            if "gov.uk/guidance" in url:
                dummy_dict = scrape_govuk_guidance(url)
                # extract the relevant entries
                scrape_dict.update(
                {key : dummy_dict[key] for key in
                 ["title","text_dump","text_segmented","hyperlinks_dump","timestamp"]
                }
            )
            # fall back to raw text extraction without segmentation
            # if URL does not point to a "gov.uk/guidance" resource
            else:
                scrape_dict.update(scrape_html_raw(url))
        else:
            warnings.warn("Cannot scrape documents of type \"" + str(media_type) + "\"" )
            scrape_dict["timestamp"] = timestamp_utc()

        scraped_contents.append(scrape_dict)

    return scraped_contents
