{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels\n",
    "Here we take the dataframes of scraped pdfs and htmls and create a new dataframe for each of them.\n",
    "We create a new dataframe for each html document that has additional columns for general labels (\"keywords\"),\n",
    "labels for the needed paperwork (\"needed_records\"), and\n",
    "labels for references to other documents (\"references\" and \"reference urls\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Changes: cell 10 and 11 are new, there is a change in cell 21 (the former cell 19),\n",
    "all cells after cell 21 (the examples) are deleted\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###------- TODO: change ------------\n",
    "# pip freeze?:\n",
    "#!pip install https://github.com/elyase/geotext/archive/master.zip\n",
    "from keywords import * \n",
    "###-------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os, re\n",
    "from geotext import GeoText\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproduceability use the same path as where the nationbetter \n",
    "# package stored the data!\n",
    "# output_path = '~/S2DS/Nation.Better/nationbetter_data'\n",
    "# output_path = os.path.expanduser(output_path)\n",
    "output_path =  os.path.abspath(os.path.join('..','..','..','nationbetter_data'))\n",
    "\n",
    "formatted_html_path = os.path.join(output_path,'formatted_html_dfs')\n",
    "raw_html_dicts = os.path.join(output_path,'raw_html_dicts')\n",
    "formatted_html_pkls = os.listdir(formatted_html_path)\n",
    "html_df_pickles = os.listdir(formatted_html_path)\n",
    "html_raw_dict_pickles = os.listdir(raw_html_dicts)\n",
    "raw_pdf_dicts = os.path.join(output_path,'raw_pdf_dicts')\n",
    "pdf_raw_dict_pickles = os.listdir(raw_pdf_dicts)\n",
    "formatted_pdf_path = os.path.join(output_path,'formatted_pdf_dfs')\n",
    "formatted_pdf_pkls = os.listdir(formatted_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all pdfs\n",
    "pdfs =[]\n",
    "for file in formatted_pdf_pkls:\n",
    "    df = pickle.load(open(os.path.join(formatted_pdf_path,file),'rb'))\n",
    "    pdfs.append(df)\n",
    "    df[df['page_no']==1]\n",
    "\n",
    "for df in pdfs:\n",
    "    title_idx =df[(df['text_type']=='title')&(df['page_no']==1)].index[0]\n",
    "    has_sub = df[(df['text_type']=='subtitle')&(df['page_no']==1)].index\n",
    "    if title_idx != 0:\n",
    "        df.drop([title_idx-1],inplace = True)\n",
    "    if list(has_sub):\n",
    "        if has_sub[0]>title_idx:\n",
    "            new_title= df.iloc[title_idx].text + df.iloc[has_sub[0]].text\n",
    "            df.at[0,'text']= new_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of pdf URLs\n",
    "pdf_urls = [pdfs[i]['url'][1] for i in range(len(pdfs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in pdfs:\n",
    "    title_index = df[df['text_type']=='title'].index.min()\n",
    "    df.drop(df[~(df.index>=title_index)].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part of the code fixes a bug dropping the \"empty\" subsection counter columns in \n",
    "# pdfs[0] = Tier 2 Policy Guidance. There are however more issues and this fix should\n",
    "# be implemented on a deeper leve\n",
    "keylist = []\n",
    "for df in pdfs:\n",
    "    keylist.append(df.keys())\n",
    "keylist\n",
    "#dropping empty (sub)title columns\n",
    "for df_no in range(len(pdfs)):\n",
    "    find_sec = re.compile(r'\\w*title$')\n",
    "    find=list(filter(None,[find_sec.search(key) for key in keylist[df_no]]))\n",
    "    old_keys = [key.group() for key in find]\n",
    "    for key in old_keys:\n",
    "        if len(pdfs[df_no][key].unique())==1:\n",
    "            pdfs[df_no].drop(key,axis =1,inplace=True)\n",
    "#rebuild keylist for new columns\n",
    "keylist = []\n",
    "for df in pdfs:\n",
    "    keylist.append(df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the columns in the dataframe with the convention of html \n",
    "for df_no in range(len(pdfs)):\n",
    "    find_sec = re.compile(r'\\w*title$')\n",
    "    find=list(filter(None,[find_sec.search(key) for key in keylist[df_no]]))\n",
    "    old_keys = [key.group() for key in find]\n",
    "    #if the dataframe of pdfs[0] is fixed and keys still need to be dropped\n",
    "    # insert here some code that renames missing keys and columns\n",
    "    # ['title','subsubsection'] -> ['title','subtitle']\n",
    "    keep_keys = keylist[df_no].drop(old_keys)\n",
    "    find_sec = re.compile(r'(title)$')\n",
    "    final_keys = [find_sec.sub(r'section',key) for key in keylist[df_no]]\n",
    "    rename_keys = pd.Index(final_keys).drop(keep_keys)\n",
    "    rename_dict = dict(zip(old_keys,rename_keys))\n",
    "    pdfs[df_no].rename(columns = rename_dict,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write function that can be iterated over to obtain section title columns\n",
    "#for single df\n",
    "# df_no = 1\n",
    "# Sorting the keys into agg_keys, which will generate the column names\n",
    "# and dropping unwanted keys\n",
    "keylist = []\n",
    "for df in pdfs:\n",
    "    keylist.append(df.keys())\n",
    "\n",
    "def get_title_col_pdf(df,sec_col='text'):\n",
    "    '''\n",
    "    Taking a column (of section counters) and check weither a section\n",
    "    transition occurs. If yes store in column [(sub)*section]_title\n",
    "    the tite (assuming it happpends) on the first line of the section\n",
    "    '''\n",
    "    #Checking if the (sub)section title changes if yes return true\n",
    "    is_sec_title = (df[sec_col]-df[sec_col].shift().bfill()).astype(bool)\n",
    "    colname = sec_col + ' title'  \n",
    "    return is_sec_title, colname\n",
    "\n",
    "\n",
    "for num, df in enumerate(pdfs):\n",
    "    #get the keys\n",
    "    keylist = df.keys()\n",
    "    find_sec = re.compile(r'\\w*section$')\n",
    "    find=list(filter(None,[find_sec.search(key) for key in keylist]))\n",
    "    agg_keys = [key.group() for key in find]\n",
    "    sec_keys = agg_keys + ['page_no','text','url']\n",
    "    #reduce the dataframe\n",
    "    df = df[sec_keys]\n",
    "\n",
    "    #call function\n",
    "    for key in agg_keys:\n",
    "        is_sec_title, colname = get_title_col_pdf(df,key)\n",
    "        #Selects string column where is_sec_title true, returns section name\n",
    "        titlecol = df[is_sec_title].text.replace('\\s+', ' ', regex=True).str.strip()\n",
    "        titlecol.name = colname\n",
    "        df = pd.concat([df,titlecol],axis = 1)\n",
    "        df[colname].fillna(method='ffill',inplace=True)\n",
    "\n",
    "    #Generate dict for aggregation of text in (sub)*sections\n",
    "    dex = df.keys().drop(agg_keys+['text']) \n",
    "    agg_fn = ['first' for no_dex in range(len(dex))]\n",
    "    agg_arg = dict(zip(dex,agg_fn))\n",
    "    agg_arg.update({'text':' '.join})\n",
    "    df.groupby(agg_keys,as_index = False).agg(agg_arg)\n",
    "   # df.rename(columns={'text':'string'},inplace=True)\n",
    "    pdfs[num]=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing in all dataframes should start at 0\n",
    "pdfs = [pdfs[i].reset_index() for i in range(len(pdfs))]\n",
    "\n",
    "# list of all pdf titles\n",
    "pdf_titles = []\n",
    "for df in pdfs:\n",
    "    title_loc = df.index.min()\n",
    "    pdf_titles.append(df.text.replace('\\s+', ' ', regex=True).str.strip()[title_loc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">!!!!!!!!!!!!!!!! Changes: 2 NEW CELLS !!!!!!!!!!!!!!!!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the biggest segment has to be \"section\"\n",
    "for i in range(len(pdfs)):\n",
    "    if 'section' not in pdfs[i].columns:\n",
    "        if 'subsection' in pdfs[i].columns:\n",
    "            pdfs[i]=pdfs[i].rename(columns = {'subsection':'section'})  \n",
    "            pdfs[i]=pdfs[i].rename(columns = {'subsection title':'section title'}) \n",
    "            if 'subsubsection' in pdfs[i].columns:\n",
    "                pdfs[i]=pdfs[i].rename(columns = {'subsubsection':'subsection'})\n",
    "                pdfs[i]=pdfs[i].rename(columns = {'subsubsection title':'subsection title'})\n",
    "        elif 'subsubsection' in pdfs[i].columns:\n",
    "            pdfs[i]=pdfs[i].rename(columns = {'subsubsection':'section'})\n",
    "            pdfs[i]=pdfs[i].rename(columns = {'subsubsection title':'section title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing index column for PDFs\n",
    "for i in range(len(pdfs)):\n",
    "    pdfs[i] = pdfs[i].drop('index', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">------------------------------------------------------------------------------------------------------</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all htmls\n",
    "htmls =[]\n",
    "for file in formatted_html_pkls:\n",
    "    df = pickle.load(open(os.path.join(formatted_html_path,file),'rb'))\n",
    "    htmls.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_col_html(df,sec_col='string'):\n",
    "    #Checking if the (sub)section title changes if yes return true\n",
    "    is_sec_title = (df[sec_col]-df[sec_col].shift().bfill()).astype(bool)\n",
    "    colname = sec_col + ' title'\n",
    "    #Selects string column where is_sec_title true, returns section name\n",
    "    df[colname]= df[is_sec_title].string\n",
    "    df[colname].fillna(method='ffill',inplace = True)\n",
    "\n",
    "def agg_subsections_html(df):\n",
    "    #get columns which need to be added \n",
    "    list_to_new_keys = ['section','subsection']\n",
    "    for key in list_to_new_keys:\n",
    "        get_title_col_html(df,key)\n",
    "    df.groupby(['section','subsection'],as_index = False).agg({'string':' '.join,'section title':'first','subsection title':'first'},inplace = True)\n",
    "\n",
    "for df in htmls:\n",
    "    agg_subsections_html(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column \"string\" in the html dataframes as \"text\"\n",
    "for i in range(len(htmls)):\n",
    "    htmls[i].rename(columns = {'string':'text'}, inplace = True)\n",
    "\n",
    "# html titles and URLs\n",
    "html_titles = []\n",
    "html_urls = []\n",
    "for i in range(len(htmls)):\n",
    "    raw_dict = pickle.load(open(os.path.join(raw_html_dicts,html_raw_dict_pickles[i]),'rb'))\n",
    "    html_titles.append(raw_dict[\"title\"])\n",
    "    html_urls.append(raw_dict[\"URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the columns for the labels are added, some entries are set to empty lists\n",
    "\n",
    "# for html\n",
    "for i in range(len(htmls)):\n",
    "    htmls[i]['document title'] = html_titles[i]    \n",
    "    htmls[i]['document url'] = html_urls[i]\n",
    "    htmls[i]['document type'] = \"html\" \n",
    "    htmls[i]['keywords'] = [[] for _ in range(len(htmls[i]))]\n",
    "    htmls[i]['needed records'] = [[] for _ in range(len(htmls[i]))]\n",
    "    htmls[i]['references'] = [[] for _ in range(len(htmls[i]))]\n",
    "    htmls[i]['reference urls'] = [[] for _ in range(len(htmls[i]))]\n",
    "\n",
    "# for pdf\n",
    "for i in range(len(pdfs)):\n",
    "    pdfs[i]['document title'] = pdf_titles[i]\n",
    "    pdfs[i]['document url'] = pdf_urls[i]\n",
    "    pdfs[i]['document type'] = \"pdf\" \n",
    "    pdfs[i]['keywords'] = [[] for _ in range(len(pdfs[i]))]\n",
    "    pdfs[i]['needed records'] = [[] for _ in range(len(pdfs[i]))]\n",
    "    pdfs[i]['references'] = [[] for _ in range(len(pdfs[i]))]\n",
    "    pdfs[i]['reference urls']=[[] for _ in range(len(pdfs[i]))]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword lists and dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find mentioning of \"Appendix XYZ and \"Part XYZ\"\n",
    "pattern_appendix = r\"[Aa]ppendix [0-9]*[A-Za-z]*[-]*\\s*[A-Za-z]*\\s*[(]*[A-Za-z]*\\s*[A-Za-z]*[)]*\"\n",
    "pattern_part = r\"[Pp]art [0-9]*[A-Za-z]*\\s*[A-Za-z]*\\s*[(]*[A-Za-z]*\\s*[A-Za-z]*[)]*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list of all document titles is updated with all expressions from pattern_appendix\n",
    "# and pattern_part that appear in the list of documents\n",
    "\n",
    "ref_in_title_list = html_titles + pdf_titles\n",
    "\n",
    "for i in range(len(htmls + pdfs)):\n",
    "    \n",
    "    ref_appendix = re.findall(pattern_appendix, str(ref_in_title_list[i]) )\n",
    "    ref_part = re.findall(pattern_part, str(ref_in_title_list[i]) )\n",
    "    \n",
    "    if ref_part!=[]:\n",
    "        ref_in_title_list+= ref_part\n",
    "     \n",
    "    if ref_appendix!=[]:\n",
    "        ref_in_title_list+= ref_appendix\n",
    "        \n",
    "    else: \n",
    "        continue\n",
    "        \n",
    "keyword_processor_ref.add_keywords_from_list(ref_in_title_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"keywords\", \"needed records\" and \"references\" columns are updated with all the keywords found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords for htmls\n",
    "for i in range(len(htmls)):\n",
    "    doctitle=html_titles[i]\n",
    "    for j in range(len(htmls[i])):       \n",
    "        sectitle=str(htmls[i]['section title'][j])\n",
    "        subsectitle=str(htmls[i]['subsection title'][j])\n",
    "        # more weight is put to words appearing in titles\n",
    "        text=3*(doctitle + \" \")  + 2*(sectitle + \" \")+ subsectitle + \" \" +htmls[i]['text'][j]\n",
    "        #The regional keywords are updated with all names of countries except \"United Kingdom\"\n",
    "        countrylist=list(Counter(GeoText(text).countries).items())\n",
    "        count_minus_uk = [t for t in countrylist if (t[0] != 'United Kingdom')] \n",
    "        keywords_all=list(Counter(keyword_processor_all.extract_keywords(text)).items())\n",
    "        htmls[i]['keywords'][j]=keywords_all +count_minus_uk\n",
    "        keywords_rec=list(Counter(keyword_processor_rec.extract_keywords(text)).items())\n",
    "        htmls[i]['needed records'][j]=keywords_rec\n",
    "        keywords_ref = list(Counter(keyword_processor_ref.extract_keywords(text)).items())\n",
    "        # removing self-references\n",
    "        keywords_refminusself = [t for t in keywords_ref if (t[0] not in subsectitle) if (t[0] not in sectitle) if (t[0] not in doctitle)]\n",
    "        htmls[i]['references'][j]=keywords_refminusself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords for pdf\n",
    "\n",
    "for i in range(len(pdfs)):\n",
    "    doctitle = pdf_titles[i]\n",
    "    for j in range(len(pdfs[i])):\n",
    "        sectitle = \"\"\n",
    "        if 'section title' in pdfs[i].columns:\n",
    "            sectitle = str(pdfs[i]['section title'][j])\n",
    "        subsectitle = \"\"\n",
    "        if 'subsection title' in pdfs[i].columns:\n",
    "            subsectitle = str(pdfs[i]['subsection title'][j])\n",
    "        subsubsectitle = \"\"\n",
    "        if 'subsubsection title' in pdfs[i].columns:\n",
    "            subsubsectitle = str(pdfs[i]['subsubsection title'][j])\n",
    "        subsubsubsectitle = \"\"\n",
    "        if 'subsubsubsection title' in pdfs[i].columns:\n",
    "            subsubsubsectitle = str(pdfs[i]['subsubsubsection title'][j])\n",
    "        subsubsubsubsectitle = \"\"\n",
    "        if 'subsubsubsubsection title' in pdfs[i].columns:\n",
    "            subsubsubsubsectitle = str(pdfs[i]['subsubsubsubsection title'][j])\n",
    "        # more weight is put to words appearing in titles\n",
    "        text = (pdfs[i]['text'][j]  + \" \" + 3*(doctitle + \" \")  + 2*(sectitle + \" \")\n",
    "                + subsectitle + \" \" + subsubsectitle + \" \" \n",
    "                + subsubsubsectitle + \" \" + subsubsubsubsectitle) \n",
    "        #The regional keywords are updated with all names of countries except \"United Kingdom\"\n",
    "        countrylist=list(Counter(GeoText(text).countries).items())\n",
    "        count_minus_uk = [t for t in countrylist if (t[0] != 'United Kingdom')] \n",
    "        keywords_all=list(Counter(keyword_processor_all.extract_keywords(text)).items())\n",
    "        pdfs[i]['keywords'][j]=keywords_all +count_minus_uk\n",
    "        keywords_rec=list(Counter(keyword_processor_rec.extract_keywords(text)).items())\n",
    "        pdfs[i]['needed records'][j]=keywords_rec\n",
    "        keywords_ref = list(Counter(keyword_processor_ref.extract_keywords(text)).items())\n",
    "        # removing self-references\n",
    "        keywords_refminusself = [t for t in keywords_ref if (t[0] not in subsectitle) if (t[0] not in sectitle) if (t[0] not in doctitle)]\n",
    "        pdfs[i]['references'][j]=keywords_refminusself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update column \"reference urls\" with urls corresponding to titles that appear in the \"references\" column\n",
    "\n",
    "all_titles = html_titles + pdf_titles\n",
    "all_urls = html_urls + pdf_urls\n",
    "\n",
    "#for html documents\n",
    "for i in range(len(htmls)):\n",
    "    for j in range(len(htmls[i])):      \n",
    "        tuple_list= htmls[i]['references'][j]\n",
    "        url_for_ref = []\n",
    "        if tuple_list != []: \n",
    "            value_list = [itm[0] for itm in tuple_list]\n",
    "            for ref in  value_list:\n",
    "                for longtitle in all_titles:\n",
    "                    if ref in longtitle:\n",
    "                        idx = all_titles.index(longtitle)\n",
    "                        item = all_urls[idx]\n",
    "                        url_for_ref.append(item)\n",
    "        htmls[i]['reference urls'][j] = url_for_ref\n",
    "\n",
    "#for PDFs\n",
    "for i in range(len(pdfs)):\n",
    "    for j in range(len(pdfs[i])):  \n",
    "        tuple_list= pdfs[i]['references'][j]\n",
    "        url_for_ref = []\n",
    "        if tuple_list != []: \n",
    "            value_list = [itm[0] for itm in tuple_list]\n",
    "            for ref in  value_list:\n",
    "                for longtitle in all_titles:\n",
    "                    if ref in longtitle:\n",
    "                        idx = all_titles.index(longtitle)\n",
    "                        item = all_urls[idx]\n",
    "                        url_for_ref.append(item)\n",
    "        pdfs[i]['reference urls'][j] = url_for_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle resulting list of dataframes\n",
    "We create a combined list that contains both the pdf and the html dataframes.\n",
    "This listed is pickled as 'labeled_immigration_rules.pickle'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">!!!!!! CHANGES: in next cell</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first pdf has such a different format that we have to leave it out for now.\n",
    "\n",
    "list_of_dataframes = pdfs[:1] + htmls\n",
    "\n",
    "with open('labeled_corpus.pickle','wb') as f:\n",
    "    pickle.dump(list_of_dataframes,f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">CHANGES: examples deleted</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
