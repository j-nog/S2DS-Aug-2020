{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extracting Important Keywords by Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../input/2020-07-16_Tier-2-5-sponsor-guidance_Jul-2020_v1.0_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_doc(doc):\n",
    "    \"\"\"\n",
    "    Cleans the document from unneecessary chars/words, etc.\n",
    "    \n",
    "    TODO : maybe first-level / second-level cleaning\n",
    "    \"\"\"\n",
    "    doc = re.sub(r\"[\\t\\n]+\", \"\", doc)                                                   # find & replace \\t and \\n with empty string\n",
    "    doc = re.sub(r\"[^\\x00-\\x7F]+\", \" \", doc)                                            # remove non-ascii chars\n",
    "    doc = re.sub(r\" +\", \" \", doc)                                                       # remove dublicate spaces\n",
    "    doc = doc.strip()                                                                   # strip leading/trailing spaces\n",
    "    doc = re.sub(r\"(Page)\\s\\d{1,2,3}\\s\\w+\\s\\d{1,3}\\s(Tiers 2 and 5: guidance for sponsors - version 07/20)\", \"\", doc)  # TODO\n",
    "    doc = re.sub(r\"(Annex)\\s(\\w)\", r\"\\1_\\2\", doc)                                       # find & replace Annex 9 -> Annex_9 \n",
    "    doc = re.sub(r\"(Appendix)\\s(\\w)\", r\"\\1_\\2\", doc)                                    # find & replace Apeendix 9 -> Appendix_9 \n",
    "    doc = re.sub(r\"(Table)\\s(\\w)\", r\"\\1_\\2\", doc)                                       # find & replace Table 9 -> Table_9\n",
    "    doc = re.sub(r\"(Tier|Tiers)\\s(\\d)\\s(and|or|and/or)\\s(\\d)\", r\"\\1_\\2_\\3_\\4\", doc)     # find & replace Tier 2 and 5 -> Tier_2_and_5\n",
    "    doc = re.sub(r\"(Tier)\\s(\\d)\", r\"\\1_\\2\", doc)                                        # find & replace Tier 4 -> Tier_4\n",
    "    doc = re.sub(r\"(\\d{1,2})\\s(January|February|March)\\s(\\d{4})\", r\"\\1_\\2_\\3\", doc)     # combine dates -> 1_June_2020\n",
    "    doc = re.sub(r\"(\\d{1,2})\\s(April|May|June)\\s(\\d{4})\", r\"\\1_\\2_\\3\", doc)             # combine dates \n",
    "    doc = re.sub(r\"(\\d{1,2})\\s(July|August|September)\\s(\\d{4})\", r\"\\1_\\2_\\3\", doc)      # combine dates \n",
    "    doc = re.sub(r\"(\\d{1,2})\\s(October|November|December)\\s(\\d{4})\", r\"\\1_\\2_\\3\", doc)  # combine dates \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages :  209\n"
     ]
    }
   ],
   "source": [
    "df.raw_text = df.raw_text.apply(lambda x:clean_doc(x))\n",
    "docs = df.raw_text.to_list()\n",
    "print (\"Number of pages : \" , len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# dummy list of keywords to be EXCLUDED\n",
    "list_of_keywords = ['from', 'subject', 're', 'edu', 'use'] \n",
    "stop_words.extend(list_of_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How to Use Tfidftransformer & Tfidfvectorizer?\n",
    "\n",
    "Term Frequency- Inverse Document Frequency (IDF) takes the sparse matrix from CountVectorizer to generate the IDF when you invoke fit. **An extremely important point to note here is that the IDF should be based on a large corpora and should be representative of texts you would be using to extract keywords.**\n",
    "\n",
    "https://github.com/kavgan/nlp-in-practice/blob/master/tf-idf/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb\n",
    "\n",
    "Scikit-learn’s **Tfidftransformer** and **Tfidfvectorizer** aim to do the same thing, which is to convert a collection of raw documents to a matrix of TF-IDF features. The differences between the two modules can be quite confusing and it’s hard to know when to use which. This article shows you how to correctly use each module, the differences between the two and some guidelines on what to use when.\n",
    "\n",
    "https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.Xz_UPegzaXI\n",
    "\n",
    "So now you may be wondering, why you should use more steps than necessary if you can get everything done in two steps. Well, there are cases where you want to use Tfidftransformer over Tfidfvectorizer and it is sometimes not that obvious. Here is a general guideline:\n",
    "\n",
    "* If you need the term frequency (term count) vectors for different tasks, use **Tfidftransformer**.\n",
    "* If you need to compute tf-idf scores on documents within your “training” dataset, use **Tfidfvectorizer**\n",
    "* If you need to compute tf-idf scores on documents outside your “training” dataset, use either one, **both** will work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Tfidftransformer Usage\n",
    "### 3.1. fit_transform WHOLE document, then extract keywords in PAGE / SECTION(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "word_count_vector (document size)  :  209\n",
      "word_count_vector (distinct words) :  3445\n",
      "10 words from our vocabulary       :  ['tier_2_and_5', 'addendum', 'published', '19_july_2019', 'updated', '03', '19', 'replaced', '17_july_2019', 'applies']\n",
      "get feature names                  :  ['annex_7', 'annex_8', 'annex_9', 'annex_provides', 'annexes', 'announced', 'announcements', 'annual', 'annum', 'another']\n",
      "vectors found in this page         :  (1, 3445)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>system</th>\n",
       "      <td>0.239034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.219803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eea</th>\n",
       "      <td>0.196637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sponsor</th>\n",
       "      <td>0.196481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>0.169944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>licence</th>\n",
       "      <td>0.151065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>based</th>\n",
       "      <td>0.147372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0.142097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>0.141099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immigration</th>\n",
       "      <td>0.134588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tfidf\n",
       "system       0.239034\n",
       "come         0.219803\n",
       "eea          0.196637\n",
       "sponsor      0.196481\n",
       "trust        0.169944\n",
       "licence      0.151065\n",
       "based        0.147372\n",
       "work         0.142097\n",
       "education    0.141099\n",
       "immigration  0.134588"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# instantiate CountVectorizer() \n",
    "# create a CountVectorizer to count the number of words (term frequency)\n",
    "cv = CountVectorizer(max_df=0.85, stop_words=stop_words)\n",
    " \n",
    "# this steps generates word counts for the words in your docs (WHOLE DOCUMENTS)\n",
    "word_count_vector = cv.fit_transform(docs)\n",
    "\n",
    "# compute the IDF values by calling tfidf_transformer.fit(word_count_vector) on the word counts\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "# Once you have the IDF values, you can now compute the tf-idf scores for any document or set of documents.\n",
    "\"\"\"\n",
    "fot the moment PAGE, may be SECTION / SUBSECTION / ...\n",
    "\"\"\"\n",
    "# get the document (page) that we want to extract keywords from\n",
    "#doc = \"\".join(docs[200])\n",
    "doc = docs[15]\n",
    "\n",
    "print(type(doc))\n",
    "\n",
    "# generate tf-idf for the given document\n",
    "tf_idf_vector = tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "# you only needs to do this once\n",
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "print(\"word_count_vector (document size)  : \", word_count_vector.shape[0])\n",
    "print(\"word_count_vector (distinct words) : \", word_count_vector.shape[1])\n",
    "print(\"10 words from our vocabulary       : \", list(cv.vocabulary_.keys())[:10])\n",
    "print(\"get feature names                  : \", list(cv.get_feature_names())[500:510])\n",
    "print(\"vectors found in this page         : \", tf_idf_vector.shape)\n",
    "\n",
    "# get the vector in this page, since doc is a single page, idx=0\n",
    "first_document_vector = tf_idf_vector[0]\n",
    " \n",
    "# print the scores \n",
    "dff = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, \n",
    "             columns=[\"tfidf\"]).sort_values(by=[\"tfidf\"],ascending=False).head(10)\n",
    "\n",
    "# result = dff.to_dict()\n",
    "dff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. fit_transform PAGE document, then extract keywords in PAGE / SECTION(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_count_vector (document size)  :  16\n",
      "word_count_vector (distinct words) :  129\n",
      "10 words from our vocabulary       :  ['applying', 'licence', 'sponsorship', 'tiers_2_and_5', 'points', 'based', 'system', 'primary', 'immigration', 'routes']\n",
      "get feature names                  :  []\n",
      "vectors found in the first page    :  (16, 129)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>applying</th>\n",
       "      <td>0.651225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sponsorship</th>\n",
       "      <td>0.582716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>licence</th>\n",
       "      <td>0.486157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provider</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>points</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poses</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>primary</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>principles</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>providers</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tfidf\n",
       "applying     0.651225\n",
       "sponsorship  0.582716\n",
       "licence      0.486157\n",
       "07           0.000000\n",
       "provider     0.000000\n",
       "points       0.000000\n",
       "poses        0.000000\n",
       "primary      0.000000\n",
       "principles   0.000000\n",
       "providers    0.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# get page\n",
    "doc = \"\".join(docs[15])\n",
    "\n",
    "# sentence tokenizer\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "punkt_param = PunktParameters()\n",
    "abbreviation = ['f', 'fr', 'k', 'u.k.', 'gov.uk.', 'fig']\n",
    "punkt_param.abbrev_types = set(abbreviation)\n",
    "tokenizer = PunktSentenceTokenizer(punkt_param)\n",
    "tokenizer.train(doc)\n",
    "doc = tokenizer.tokenize(doc)\n",
    "\n",
    "# instantiate CountVectorizer() \n",
    "# create a CountVectorizer to count the number of words (term frequency)\n",
    "cv = CountVectorizer(max_df=0.85, stop_words=stop_words)\n",
    "\n",
    "# this steps generates word counts for the words in your docs (WHOLE DOCUMENTS)\n",
    "word_count_vector = cv.fit_transform(doc)\n",
    "\n",
    "# compute the IDF values by calling tfidf_transformer.fit(word_count_vector) on the word counts\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "# Once you have the IDF values, you can now compute the tf-idf scores for any document or set of documents.\n",
    "\"\"\"\n",
    "fot the moment PAGE, may be SECTION / SUBSECTION / ...\n",
    "\"\"\"\n",
    "# get the document (page) that we want to extract keywords from\n",
    "\n",
    "# generate tf-idf for the given document\n",
    "tf_idf_vector = tfidf_transformer.transform(cv.transform(doc))\n",
    "\n",
    "# you only needs to do this once\n",
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "print(\"word_count_vector (document size)  : \", word_count_vector.shape[0])\n",
    "print(\"word_count_vector (distinct words) : \", word_count_vector.shape[1])\n",
    "print(\"10 words from our vocabulary       : \", list(cv.vocabulary_.keys())[:10])\n",
    "print(\"get feature names                  : \", list(cv.get_feature_names())[500:510])\n",
    "print(\"vectors found in the first page    : \", tf_idf_vector.shape)\n",
    "\n",
    "# get the vector in this page, since doc is a single page, idx=0\n",
    "first_document_vector = tf_idf_vector[0]\n",
    " \n",
    "# print the scores \n",
    "dff = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, \n",
    "             columns=[\"tfidf\"]).sort_values(by=[\"tfidf\"],ascending=False).head(10)\n",
    "\n",
    "# result = dff.to_dict()\n",
    "dff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tfidfvectorizer Usage\n",
    "### 4.1. fit_transform WHOLE document, then extract keywords in PAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>system</th>\n",
       "      <td>0.239034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.219803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eea</th>\n",
       "      <td>0.196637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sponsor</th>\n",
       "      <td>0.196481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>0.169944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>licence</th>\n",
       "      <td>0.151065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>based</th>\n",
       "      <td>0.147372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0.142097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>0.141099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immigration</th>\n",
       "      <td>0.134588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tfidf\n",
       "system       0.239034\n",
       "come         0.219803\n",
       "eea          0.196637\n",
       "sponsor      0.196481\n",
       "trust        0.169944\n",
       "licence      0.151065\n",
       "based        0.147372\n",
       "work         0.142097\n",
       "education    0.141099\n",
       "immigration  0.134588"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    " \n",
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, max_df=0.85, stop_words=stop_words) \n",
    " \n",
    "# just send in all your docs here \n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer = tfidf_vectorizer_vectors[15] \n",
    " \n",
    "# place tf-idf values in a pandas data frame \n",
    "dff = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), \n",
    "             columns=[\"tfidf\"]).sort_values(by=[\"tfidf\"],ascending=False).head(10)\n",
    "\n",
    "# result = dff.to_dict()\n",
    "dff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. fit_transform PAGE document, then extract keywords in PAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>applying</th>\n",
       "      <td>0.651225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sponsorship</th>\n",
       "      <td>0.582716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>licence</th>\n",
       "      <td>0.486157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provider</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>points</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poses</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>primary</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>principles</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>providers</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tfidf\n",
       "applying     0.651225\n",
       "sponsorship  0.582716\n",
       "licence      0.486157\n",
       "07           0.000000\n",
       "provider     0.000000\n",
       "points       0.000000\n",
       "poses        0.000000\n",
       "primary      0.000000\n",
       "principles   0.000000\n",
       "providers    0.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    " \n",
    "# get page\n",
    "doc = \"\".join(docs[15])\n",
    "\n",
    "# sentence tokenizer\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "punkt_param = PunktParameters()\n",
    "abbreviation = ['f', 'fr', 'k', 'u.k.', 'gov.uk.', 'fig']\n",
    "punkt_param.abbrev_types = set(abbreviation)\n",
    "tokenizer = PunktSentenceTokenizer(punkt_param)\n",
    "tokenizer.train(doc)\n",
    "doc = tokenizer.tokenize(doc)\n",
    "\n",
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, max_df=0.85, stop_words=stop_words) \n",
    " \n",
    "# just send in all your docs here \n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(doc)\n",
    "\n",
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer = tfidf_vectorizer_vectors[0]\n",
    " \n",
    "# place tf-idf values in a pandas data frame \n",
    "dff = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), \n",
    "             columns=[\"tfidf\"]).sort_values(by=[\"tfidf\"],ascending=False).head(10)\n",
    "\n",
    "# result = dff.to_dict()\n",
    "dff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
